 Tere tulemast Luangus no 11. Vist, kui ma ei ette mäljata, et täna räägime peamised Kubernetesest, aga natuke naalguses lõpetame võtla kokku ka mikropeenosti teema, kuna ehmise luangus läks natuke kiireks lõpupoole ja mul on plaani saanud kui paaresse veel ülekata enne kui me Kubernetesi juurde lähme, et me tõi rääkuseme konteineriseerimist puudustest ja sellistest olukorrast, mida Kubernetes laadne platform võiks parandatadame ja kordama sell otsa üle uuotski. Ajalooliselt on kokus veekitehnoloogiat ja üldse rakendust arendus liikunud edasi, et kui alguses pike rakendati sellist waterpool stiilis arendust, kus pandi nõuded algust paika ja arendati siis rakendus valmis enam päeha mahtal tõuetele, ja siis kile vaadate üle, et kas on mingisubuseid asju vaja ümber arendada, siis 2000-leidel alustati sellist agiliselt lähenemist, kus räägiti läbi koko aega näit klientidega, et saaks tagas, et nii kiresti kui võimalik, ja et oleks võimalik rakenduse version iga natuke saadagant ümber arendada ja siis klientile testimava hakkad, et ei tekki teda nõnu sellist ühti versiooni rakendusest, ma ei tekki palju versiooni, mida siis näidatud klientidele ajas ja rakendat edaselt. Tänapäeval prooviteks sa rakendada DevOpsi, mis toimistatud on samamist agile, aga ta paled natuke rangel on paika, mis toimub iga versiooni arendus käigus ja prooviteks sa teha niimoodi, et need versioonipe vahed toimub hästi-hästi kiresti ja ei ole proovleemi, kui näiteks mõnes asutuses teha mitte versiooni päevas ja laitaks üles ja lastaks vaadata, kas klientidele tuleb negatiivsi tagasliseid või puhtad meetrika või logidebojal näeb, et midagi on uute versioonte ikas proovleemi maatinda. Waterfall aretab sellist monoleiselt serverid, vahendid üldiselt üles püüsiinistel serverid, et ma olen suht rakendiselt üldiselt serveridele ja Anne Keskus keskus firmad pigem ehitsid oma Anne Keskus ja tain oma püüsiinistel serveridele üles. Ma olen me varasemata suhagustus läheb, et mitme iilised, ent iilised arkitektuurid, aga need hakkab nii siis 2000-tal aastat aane üles, aga virtuaalserverid keskus on nii firma, kes pakkusti näiteks Eestis ja sa on võimalast oma, nii misest ma ei eest kuidu, PHP-versioalsvasserverid, et saad oma rakendise sinna üles teada ja siin hakkas olema see, et sa ei ole oksutas seda enam ennaseerverid, aga nii misubise teise teenust pakku ja serverid ja virtuaalserverid. Ja umbes 2008-2009-2010 juba enne 2010 hakkas pilnpopulaaseks saama ja teavad, et kasutat jalgusev võrkaalmasinaidid kontainerid, et tuli tegelikult natukane hiljem, aga selle läpkel hakkas RNN-naga mikro teenuste maailmed hakkati järgis rokkem rakendu siis nii enampeal kihilisena, või niimoodi iga hiht oli ka jäägatud eraldi nagu sellistekse loogika järgide eraldi teenusteks, et võite ette kuududa, et oli üks root, siis tegisid ripas, erid on pige nagu maatrix rakendusja aranud, et oleks aga mikro teenuste maatrixed. Ja tänapäeval peab kõik asjad saata külleks kontainerit, et see isegi kui peamine eesmärka virtuaalmasineta sõle seda, siis on tegelikult ikkagi hea virtuaalmasineta eeskohdata kontainerit, kas või isoleerimise või keskkondade eraldamise eesmärkina. Ja tegelikult see mikro teenuste arend on ka aidanud ka agiilsest arenduseks DevOps arenduse poole. Selle tõttu, et ühest vaatest on vajalik automatiseerida arenduseks rakenduse üles, seal, mis kui rakenduseks olnud 16 tükist või 25 tükist, et siis ei ole hea, kui käsitsi pannaks asju üles, et parem on automatiseerida ja kogu selle DevOps mõnesedlads mõtte ongi, et võimalikult palju iga kiilsse uue versiooni ülesseadmine automatiseerida niimoodi ära, et seda ei peaks inimene käsitsi konfigureerima, vaid iga kord, kui lasteks välja uus release, siis automaatselt või panaks üles ta siis targvara rakendi sisse. Et sügisel on kõik DevOps aine, see on varasemalt, teaks sul on teie õppakavas järgmise aasta, pakkat udenkit, ta ei ole pande õppakavas, aga teie õppakavas, ta tõenab, et see on, et sa on mõeldud, et pakkat udenkit võtavad seda võib-olla kolmandal aastalega võimõttel, teaks seda saama etteks sügisel DevOps aine põtla. Kui võtavad vast, siis teist ma kistis. Tänna ma kistis, aga aine, ette õppakavas sa panen, et teed olevad ainult alati niimoodi sisse astuni hetke. Kuna teile sisse astusite, kui ta oli sellel hetkel teile õppakavas, siis ta on teie õppakavas. Isegi kui keegi muudab seda järgmise aasta ära, et ta enam ei ole õppakavas, siis tavalselt ei muudab seda tagant järgi, vaid muudetakse uutele sisse astunutele. Sest muudetakse olukorrad, et sinna astusid sisse arvasid, et sul on mingisest specialiseerilis moodul, järg kui kaab seda. Seda üldiselt ei tehta. Ära vaata kõige viimast õppakavad versiooni, vaata seda õppakavad versiooni, kuna sina sisse astus. Ja seal ta peaks allaks alati. Kui te näete õis, siis te näete, et pakka õppakavasid ongi iga aasta kohta uus versioon. Iga aasta kohta on versioon, iga aasta nad natukene muutuvad. Ai on kõik võtta? Ai on küsitavus. Kui on täiesti mavistri aine, siis on prioriteet mavistri tudenkitele. Võib-olla te ei olegi lupatud ise registreerida. Ta võibki ole täiesti kindi pakka tudenkitele. Aga see DevOps aine ei tohiks olla kindi. Järgmine SMS seda õppatatakse hapelt see inimeste poolt. Ja neil on rohkem inimesi ka, et seda õppatada nii, et nad tõanast tead rütmest juurda. Pähemalt see on esimene plaad. Me oleme nii liikud mono rakenuses mitmekihiliselle ja servisorientad arkti-pektuurile, kus me jagame asja peab mikrovenusteemist. Nüüd hakkame panema üles konteeneritena. Aga meil on üks asja veel, mida meil ei ole vaatanud, seda vahvame järgmine lai. Kui me lähme mikrovenuste konteeneritest jooks, mis edasi meednäisemale tasemeljadise nanovenuste, võib pole kõige parem nimetada naovenusteks, sest nad on tegelikult isegi erineva mustriga. Kui me konteenerit jooksutame pidevalt, et jätame tagstall jooks, nagu webiserverid, siis serverless ja nanovenuste jooksutateks ainult siis, kui tegelikult on vaja käilitada jäänna ja jääma jooksult augstalla. Aga sellest me räägime, mis järgmise on vahvast. Ennakoodi ma tõin sellise väga lihtsa mikrovenuste näite. Tegelikult meie praktikum rakendus muutub selliseks aine lõpuks. Praegu, mis me oleme teinud, me oleme jaganud oma rahmatukohaltus alti kaheks mikrovenusteks. Selle nägaval me paneme juud erasli sellise komponenti, milleks hakkab olema frontend. Me ei nimetakse seda mikrovenusteks, aga põhimõtteliselt me paneme teha konteineristööle, aga tulelikult me paneme ta Asures üles Statik Web viiteno sõne, kui te elmisteste praktiku, mides olete lihtsalt html lehti ülespannud Asures Statik web saiti. See nädal me teeme oma appi haltuse frontendi, veel Asures üles ei pane, aga järgmiste praktiku, mides te panete sellega eraldi teenusõne. Me oleme juba hetkel kasutanud Asure 5Storages selleks, et koida rahmatukohalte. Ja kui te palete sellega frontendi, juurde tänases praktiku, mis me omses, siis tegib see üleminn asa ja järgmine näedame TVB juha speciaalse funktion juurde, mis töötab sellise taustal eventit tänalt käegitava funktionina, mis iga kord, kui Asure 5Storages laitaks ule suus rahmatukohalte, siis meie funktion automaast käibidiga uue rahmatukohalte ja kommenterid rahmatu tegisfaiididele seks. Me teeme siis järgmine nädal sellise juurde. Ja ainult lõpuks paneme sellega kahe nädalapärast üles, siis täiesti TVB-liis lahendusena, kui kasutajad saab kas frontendi otsa kasutuda või rahmatud appid otsa kasutuda. Aga idee on siis selline, et ta töötab täiesti sehase Asure 5Storages. Panuma, vaate käige üle, et oma Asure predit ei ole täieste erakasutan, nii meie läheb veel viimases loendusasulid vaja. See nädal me võib-olla natuke jääb teame sellase Asure 5Storages kasutamist, aga justing tõrjutavad masinapre konteeleerit meie veel üles ja pane. Ja viimases praksus ma natuke ka hoiata, et me paneme kaks appid tohkelikonteeleerit üles. Ja see läheb paks. Nii et see maksad krediiti ja me peame sellega natuke ettevaatek olema, et me proovime selle võimalikult kiiresti ärahinnata need viimased praktikumid, aga pärast seda, sest te panest seda asjad kinni, sest muidude, et jookseb krediit ühehtude ja selle suveks tööle jätate. Nii et me anname, kas see viimases praksis tagasi, et asjad kiiresti kinni panest ja tööle jätate suveks. See aasta ka justus paar korv, et ma jätu juba krediiti elmistes aine, et ei ole elmistes aine, et ei sõra kasutada võisteltita probleem. Mikro-teenosti ja kokku võteks räägiks korraks uuesti üle näid mikro-teenosti elised ja puudused. Üks parem, pige tugemama elis, et miks mikro-teenosti kasutatakse, on nende lihtsam hooltatavus, et me ei pea hooltama nagu tervet monoliitikorraga või monoliitimooduleid, et me saamegi neid vaadata väikse projekte ja nende väikeste projektide omanikud või arendaad. Nende on lihtsam hoidad terve projekti ülevaadad ja teada saast projektes kõik, kui projekteid on väikse. Kui on mingisugune asutuses olev monoliit, kus annebaasias on näiteks 72 tabeliit, ja siis 25 mood, et see on väga raske, et arendatel on omandavad täielliku ülevaad, et kogu terves suureks monoliitiprojektist, aga mikro-teenosti puhul on pähemalt näidest väikestest tükkitast, lihtsam täielliku ülevaadad omada ja siis on parem hooltada, parem testida ja parem kajuurutada, et me peame tekitama näiteks automaattika, mis oska ühe tocker-konteineri automaatsast ülesseada, et ei pead tegelema olukordadega, et kuidas panna üles kogu süsteem, et me saame teha nagu tükk havalt, kui me tükkid on 500 väikse. Nii et ongi, et kui teenused, mida me ülesseama on 500 väikse, see on lihtsam aru saada, lihtsam uutele töötehtel tutustada, lihtsam praktikantidele tutustada. Me ei pea nagu isegi arendust keskkondadesse hiigel suuri projekte sisse importima, et me ei vend tõmba pallamigi 3 gigabaiti paile kuskilt, või kik jama kõtti teegi tere katta, mis katavad kogu monoliidi, et ei saa parandada väikeste tükkide maha. Teenused ise üksikult vähemalt alustavad juoks, mis kiiremini, et tegelikult monoliid võib kiiremini üleskutida, kui mikroteenuste rakendus, mis koostab 60 liitopad ja viie teistimeest osast, aga nagu üksik mikroteenus kindlasti ei lähed kiiremne käima, eest sa on kiirem testida ja ei võita nii palju aega. Ja sigal te korral on ka parem isoleerimine, sest kui midagi juhtub, siis parem on vaadata logisid ja uurida nagu ühe väikse mikroteenuse konteeneri logisid, kui seal midagi juhtub, et kiiremile jääb pead üles, samas kui neid vead toimuvad mikroteenusta pahel, või kudagi on seotud mikroteenuste vaelse logika, timeoutid ja muudiga, et see sõib omakõrda nakkate kiiremise mõna. See võit ei ole etteleiselt vähendid oma teinoloogi võlga, sest näiteks, kui meil on üks mikroteenus alvasti arendatud, ja meil ei tööta hästi ja on püültanud, et see on liiga aeglene, see on suhtseb lihtne üks mikroteenus imperaarandada teise gene peale või teisid, et see on teist annebasi kasutama, kui see ei mõjutada teisi mikroteenuse, et me saaksime näiteks. MySQL asemel Postgres kasutus on võtta ühe mikroteenus raames ja see annebasi välja pahetamine ei kruudgi mõjutada ühtegi teist mikroteenustad. Siis on lihtsam, nagu tehnoloogia otsuseid teha või midagi ümber arendada ja midagi välja vahetada. Aga puuduseid on siis jälle se kogu projekti skoop või väga suureks kasvadad. Olu korras, kus meil monolidid on nii suureks, keegi seda ei mõista, sest teha on olnud, et keegi ei mõstab, et seda on mikroteenustad siiaotab kuud süsteemi ja võib juhtuda, et isegi lihtseva monolididi puhul me mõistame kõik, aga kui me jagame, et 15 tükkib, inimesed, kes kogu arhitektuurid mõistavad, et see võib omakorda raske mulle, kuna peab teatma, mis toimub võrgus, mis timeoutid peavad olema, et teenevsta vahele kui töötaks, kui kaua peaks näiteks konfigureerima kuberneeteses ningi lihtsalt, kuid palju aega andada teatud mikroteenustele, et nad üles puudivad enne, kui neid restartiteks ja igasest muud problemid hakkavad tehnuma. Kuberneeteses osast siis räägime täna. Onki, et teenustevaale on interaktioonide testi, mida on keerulisem, kui me ei testi lihtsalt koodi, vaid peame ka poolitsema, et andebaas on üleval ja kõik omavahelised mikroteenuste omavad suhtlevad ja me võime küll iga mikroteenuste üksikuna testida, aga mikroteenuste arhitektuuripuul me peame ka interaktioonideeste tegemad poolitsevad, piktuöhtevad koostus ka. Kogu süsteemi nullist ülest paneme näites uuele keskkonnas, peab omakorda olla keerulisem ja rohkem aega võtta. Ja see võib olla palju sellised sõltuvused, et andebaas ka põeva oma üleval enne, kui mingi komponent on üleval ja mingi komponent ei saa töötada enne, kui teised mikroteenustele üleval, et selt tegib omavaheliselt sõltuvus. Ja meil võib paja olla palju rohkem mänu, kui me teeme igale teenusele oma mitte jakaku keskkonnat, kui me saame seda väga hästi tegelikult ka kubaneetes konfigureerida, niimoodi, et me anname karanteeritud mäluala ja siis lubaame dal komponentidel oma karanteeritud mälualas ka rohkem mälu kasutuda, siis tegelikult me peame ikkagi oma igale teenusele alokeerima mingisuguse minimum mäluala ja monolidin ka võrel, et see võib saa tegelikult palju suurmeidu kui kõik komponenti kokku liitmed komponentide summa. Mäluala võib olla suurem kui monolidijad vaalik mänu. Eritikuna monolidine puhud meil võib olla võimalik parem ei agada indisust mälualased, et mitte mootulid tegelikult samu anmeid ojavad mänus. Ja see võib ka minna kallimaks. Eritikult me kumperäete, siis üle saame, et kubaneetes tegelikult tahab seda, et me seaksime üles ka serverid, mis haldavad kubaneeteest, et suvalise kubaneeteset lastreaks meil tavalselt jõu nagu ei piisa ühes masinas, ja selleks, et see töökaks tökke kindlalt on tegelikult vaja plastra, mis on kolm võib piis või rohkem servereid. Ma räägin tove palju sellest mahtukrohku. Ja kui me kasutame tokereid, siis tegelikult problemid, et konteinerit on nõrgevad isolatsioonid, kui virtuaalmasinate puhul, me saame seda natukene paremni parandada. Kui pärnetases, kui tokkeris, aga üldjuhul see tihti olele paremda. Et kui arenda konfigureerib valesti konteinerite seadad, siis konteinerit saavad nüüta isoleerimest. Et virtuaalmasinate puhul see võib ka juhtuda, et kui te panetid virtuaalmasinale, sisse maandite mingisugust välised kaustad, kas te ei ole te märkanud, kui te näiteks VSL-i kaud jooksutada mingit linuks virtuaalmasinat, lähete linuks virtuaalmasinat sisse ja või teil on kättesadad pig-velist poosad fallid, et teile maanditeks nagu välist poosad masina kaustad virtuaalmasine sisse. Et see on suhtseks selle default konfiguratsioon, mida tehaks, ja selleks, et teile oluks muga, et te saaksid näiteks jooksutada mingisugust käsku sellest virtuaalmasinasees ja kohem muutaneid file, mis on väljaskool, et mugavased ötku maanditeks tihki nagu välise serveri kaustad, nagu virtuaalmasinate sisse eriti nagu tavaruutiteks mitte võib olnud ilgast ja serveri keskkondudes. Aga sellise valikute tegevuse ötkus, siis ole eerem kaupõimist ära. Kui virtuaalmasinasees on lubatud näiteks, vaadata, mis on password file-is väljaskool sellest virtuaalmasinat on võimalik hakkata ära arvama parooli näiteks. See on nii sila näiteks. Võisi, et ei kasutada mingisuguste file, mille kaudus on pigi pääseb dockeris, et dockeris suusi konteneer jooksud panal minuks keskkondudes. Iga liul oluks virtuaalmasinat või konteneerid on ästi tähtiselt, et on tehaks konfigureeritud ebaefektiiselt ja dockeris on väga raske seda karanteerida, aga administreerimised asemel, et ükski tabakasutaja või arendaja näiteks kokematai seha üles konteneerid valeda õigustega, millel on õigused siis kõikki file velas pa tuge teheks. Aga kuberteete selle on see natuke panal, kun aga pereini ajal politike peale panna. Sellega, kui me vähendame nende komponentil suurlust nüüd on ülasseamad, siis me hakkame ülesseamad palju väikselt konteneerid ja mikroteemasid. See tähendab ka, et meil on täiesti tavan olukord, kui võib olla, et 50 konteneerid jooksevad serveris ja meil tuleks ülevaad, et saad, et mis nad teevad ja mis nad on, nii kaks kõik on vajalikult. Ja need, et tegelikult komponentide arvukasvuga on pihti raskem arvusadad, ega see ei ole mitkagi vajalikult jooks näiteks. Ja süssteemid peaks skaleerima ja lastselt meil tähendab see, et me lisame konteneerid, meil tegelikult oleks vajaga lisada serverid juuded, kui hetkel olevad as serverid, et see viisa selleks, et kõik konteneerid jooksitada. Ja tegelikult meil onki vaja nagu selliseid orchestreerimis lahendus, mis oskaks hakkama saada, siis paljude serverite ja võib-olla sadade konteneeritega, et siis tokkersforma kuberneetes, tänareid on teemaseks siis kuberneetes. Ma vaatan, et see on midegära ei onnustu. Kuberneetes ongi siis põhjumist, et alpenarkiis tokkersformile, kus tokkersformis on suudselt lihtne panna üste serverisse tocker, initialiseerida tokkersformi klaster ja jooksitada kahtekäsku selleks, et lisada servereid tokkersformi, niimoodi et tokkeri kaudusab siis konteneerid jooksitada paljude erinele serverite peal. Aga tegijad palju probleemi näed, et kudas halata võrke, kudas halata ketta ruumi, selliseid küsivait ketta ruume või ketta kaustasid ja sellises production keskkonast tegelikult tegid topperi kasutamiseid palju sellised väikseid probleeme. Ja kuberneetes ongi arendatud selleks, et paremini automatiseerida konteneerid rakenduste, konteneerid ülesseadmist ja konteneerid rakenduste arendusti ülesseadmist ja automatiseerimist. Ja väga suur fokus kuberneetes ongi automatiseerime, et see on nagu kõige tähtsamad omadusi, et kuberneetes on kõik võimalik ära automatiseerida, et teoreetiliselt, kui süsteemu on ülesseadud, et inimesed ei oleks enam vaja, kes seda jälgib. Ma olen ise näiteks siin Habertse keskkonnas ülesseadud IoT-annebaasi, mis jookseb kuue arvuti peal ja kasutab kuberneetes paljude komponeetid jooksutamiseks, kes on aastaid jooksud ilmal, et ma väks kuberneetes klastret jälgima või kontrolli, et nad iga natk saab ära kis puhendal karkkora peal. Kuberneetes kasvas välja, sest Google Porg projektis 2004 aastal ta anti välja alaliku projektliina 24. juunis. Talla on väga suur arvendeete keskond. Iga versioon tuleb välja iga kolme kuutakad, nii et arvendeet umbes teavad, et kui tihtima pead kuberneetes tuendama. Myötan ta muutunud natuke selliseks tasutajäritse, et seeritab tööristakse, näiteks firmadel, kes teinimad raha ja kasumid. Nendel ei ole otseselt lubatud tokkereid tasutakaasutada, aga teil oma hobi projektideaks või optingutakäigus on tokkere annad lubad suvalisel, üliopilasel või oma projektideaks seda kasutada. Siin on siis ehitatud, et infrastructure as code või infrastructure koodina tähendab seda, et oleks võimalik defineerida, mida tuleb teha konteinerides või konteinerid ülesehatmiseks koodina või templatefailjutena tavaselt Jamli või Jason mina ilma, et pead käske jooksutama. Kogu kuberneentes on selleks üleseitavud, et arendade jooksutada käske. Me jooksutada docker runni, selleks me ülespanna, vaid me kirjeltame näiteks dockerfailjis arvasalt asjad, mis peab siin üleol olema, ja me paneme need info.anme-baasi läbi atni ja üleksame, et me tahame, et sellist asjad eks justeeriks, aga me ise ei jooksut otseselt kuberneentes ja käske. Kasutateks selleks sakel mikroteenuste juurutamise automatiseerimiseks ja skaleerimiseks. Selle hetkel, kui me oleme dockerfailjid ülespanud, kuna peaks automaatselt konteinerid juurde panema või eemaldama. Ja kuidas seda eemaldamist teha niimoodi automaatselt, et ükski kasutajarroodet ei saaks. Kasutades versionid jääksid aktiivseks ja sammult, et kuidas näiteks uud versionid mikroteenuste üles, panna niimoodi, et kasutat ei märkaks midagi, et taustavalt versionid muutuvad ja üksikasutaja vea teadeti saaks. Kuberneentes ja kõige peamistolemid on poodid. Eesti kerest võib kõige kaul, ei ole kõige varem sõna, aga kõige otsasem tõlge, et mina jätkam inglisti ja see poodi termi kasutamist. Kuberneentes peamistolemid ei ole konteinerid. Kuberneentes ei seaa üles konteinerid, me seame üles kuberneentesse kauna, mille sees on üks või rohkem konteinerid. Meil on eraldi nimaruumid. Siin ide on, et konteinerid ka sees on samuti nimaruumid, aga kuberneentesse nimaruumid on pigemseleid eraldi kaunade grupid, mille saab peale panna politikaid. Meil on susteemigrup, anmebaasigrup, rakendusegrup. Me saame öelda, et rakendusegrupiil, rakenduse nimaruumil on lubatud anmebaasinimaruumi poodides päringud teha või pakete saada, aga üheleki teise nimaruumi ei ole lubatud. Me saame nimaruumid tasemelt tama regleid. Nimaruumid on mõtevan, et ühes nimaruumis poodide nimed on unikausad, aga sama nimi näiteks poskles võib olla teises nimaruums ka. Meil võib olla kaks nimaruumi, anmebaas üks ja anmebaas kaks ja mõlemus võib olla sees pood nimega poskles. Nime ruumid on siis ka nimede eraldad. Me tavaliselt ei panene poode, üksinda üles, et me ei tavaliselt, kui me näed, et run pood või deploya pood, me defineerime poode peale deploymenti, mis kirjandavad ära poodi sellised metaanme, midu koop, et sellest poodist peaks jooksutama ja muud informatsioon. Me defineerime replikaseti, mis ütleb, et deploymenti seest peab olema kolm poodi ja replikaset kontrollivs seda, et midu poodi hetkel jooksud. Ja kui me replikaseti muudame, et kolm asemel kirjutame annubaselt midu on viis, siis taustal toimuvad protsosid, mis muudavad poodid arv automaatsed ära. Meie ise ei käivita uusi poode, vaid me lihtsalt kirjutame, et midu poodi peab olema. Risaks on serviceid, kes teineerivad rootimise ja võrguregljude, kui me saame tekitada annubasi teenuse, nimetame selle Postgres teenuseks. Ja service, Postgres service, siis pireldab ära, et kui tuleb pärin Postgres service IP-adressile või hostemile, mille nimi ongi näiteks, postgres.cloud.ut.ge või sellist, siis et millistele poodistel liitus edasi suunatakse, et service on selline rootimis reglid, et kuidas suunata ümber liitust. Ja ingress on samulti selline võrgu olemis, mis defineerib ära, et kuidas välis võrgus tuleb liikus suunata milistele teenusteese. Et ingress siis defineerida näiteks hostneime, et kui on Pelle app, siis tuleb liiklus Pelle appi adressile, et siis millistele teenustele see ümber suunata. Et ingress teab siis sisevõrgu kätke saadaoks välis võrgus. Ja lisaks on sellised config mapide secretid. Secret on mingisubiselt saladuse, et näiteks saame asure, file storage, accounti, connection stringi, salmust kuberneetsse sisse saladuse. Ja siis rakennust ülesseab, mis on lööda saladuse nime, selleks, et meid eaks kasutama keskkona muutueid või mingisubist erilist viiselt kust võtta neid vääritused. Et me saame kuberneetsse ongi eraldina pole saladused, ja me saame vaelikult vääritused sisse kirjetud lihtsalt. Config map on sarane, aga ta ei ole mõeld, et saladust teab saada lihtsalt konfiguratsioonideaks. Et näiteks, kui meil on nginx konfiguratsiooni file, ja me tahame iga deploymentige seda natuke muuta, siis config mapi me saame kiihutada selle nginx konfiguratsiooni ja kasutasest mpk telt, et kudas vääritus ja muutad näiteks automaalsest lööda IP-agressid, siis mingisubuse poodi muudest meta annetest ja kirjutad see automaalsed nginx konfiguratsiooni kui selleks jaabaks. Mis on siis poodid? Poodid on põhjemusel kõige väiksemat käivitatavad üksused kuberneetsse, et ma kasutan korraks laserpointerit, et siin iga ring on üks pood, ja te näet, et selle poodi sees võib olla näiteks mitu kausta või voluumi ja mitu konteinerit, et ühe poodi sees võib olla näiteks üks konteiner või kolm konteinerid ja kaks maunditud voluumid. Ja põhimatsed igal poodil on unikaal IP-agress ja sama file system, et ühe ringi ühe poodisees olevat konteinerid kõik jagavad sama IP-agressi ja samu file, nii et nad saavad ükstese paljadele ligi. Et näiteks on näiteks, et meil on mingisemene rakendust, et me soojam ülest palja kaks procesi või kaks rakendust üks, mis tõmbab raamatuit, internetis, võib ütlemperikse tegidutamne file, kes me eraldin teema appi teeluse, mis neid pakuksis neid file kätte saada. Ja me saaksid pannada tööde nagu samas appis või samas procesis, aga me lõmmed aga kaks konteinerid, üks konteiner, mis võib tõmbab file ja teine konteiner, mis saada veri neid. Ja sellest, et nad saaksid meid filejad jagada, siis nad paremene nad ühe poodi sisse, nad jagavad sama IP-agressi ja sama samu mountitud file, nii et nad saavad. Ja need on ka sama poimust, et mälualaad nad saavad ühe omavajas suhelda, mis on maailmik olnud kõige. Nii et nad ei ole omaval isoleeritud, poimust, et ilma isoleerimata, toksultad samas nagu keskkombas. Ja skaneerimisel me siis ei muuda nagu siseliste konteiner, et arvub, et me teeme poode, et endast kogu poodist ja kogu sellest ringist, et kõik kaustad kõik konteiner, mis on endast oot skoopet ja kogu poodi kaupa. Ja idean on tegelikult seda, peaks enam päeva väitima, et parem on tegelikult ikkagi poolstas, kui on ikkagi loogiliselt eraldi mõe protsest, mis tegelikult millegad siit parem, nad ikkagi panna erinatuse poodidesse. Et ei ole nagu eesmärg, et kombineerida palju konteinerid sa vasta poodi, vaid kui otsest ranged valjadust ei ole, siis parem on ikkagi eraldi oida. Kui pärjätel, et namespecies eraldab protsestistikruppid või rühmad erinatse namespeciesse ja namespeciesse siselselt nimelt peavad olema unikaalselt, et me ei saa samase namespeciesi teha kaks poodi nimega postkas. Ja võimalda siis hakkendist keskkandi eraldadada. Aga defaultina nad ei ole isoleeritud. Et üks namespace saab teise namespeciesi ka ühenest võtta üle võrgu ja kui teie ei pana eraldi reglid peales, siis keskkondi ole eraldatud. Kui te tahad, et oleks automaatsus eraldatud, siis te peatud ülesseadma eraldi tarkkora kupernetisees, mis automaatsus konfigureerid uued namespecies niimoodi, et nendest panaks jääb paik ka näiteks võrgu poliitikad. Nii muda, et igas uueks namespecies on võrgu poliitikamisei lubav üheleki teiseks namespeciesi selle kõhendast võrtaistad. Aga võib pole käsit siin konfigureerima, et mingisest poodid saavad siiski sellest namespeciesi liidipäe suud, et kõik sellised asju saab automatiseeriga kupernetisees. Deployment, kui meid paneme poodi kupernetisees tööle ja keegi tapab poodi jära, siis poodi automaatsal ei panda uuesti tööle. Selleks, et pood oleks küsivalt jooksev, tulebki pood panna kas deploymenti sisse või stateful seti sisse või teemalsetisisse. Deployment toimimesed kirjaldabki, et see pood peab olema jooksev süsteemist. Ja ta määrad kerra sellise poodi soovitud oletu, et näiteks, et ta peab jooksma ja mitu replikat peab olema ja saab deploymentisees muuta versioone. Et näiteks, meil on deployment version 1 ja omme on deployment version 2, kus me muudame ära poodi ningisemise sissemise container image, siis automaatsalt hakkab toimuma selline taustasolev protses, mis vahetab välja kõik need jooksevad poodid deploymentisees. Deployment jääb samaks, aga deploymenti sissemiste poodide versioonid vahetades välja ja me tähaga räägime sellest, et kudas neb vahetaselt toimub. Ja on eraldi deployment kontroler peenus kuba näetases, mis jälgid kas kõik deploymentid on sellises vahelikus olekus vahe, et kui mõni pood on puudus, automaatsalt skeduleerib see suurde poodide loomist näiteks face to the node'ide peale, et kui on näha, et midagi on puudu. Poodi te arv on kolm, aga tegelikult replikeid seti järgi peaks olema viis, kui kasutel on seadistandud, et me tahame skaleerida poodide arvu viiega. Need olemid on tegelikult palju rohkem, et siia ma olime rääkinud poodist replike setist ja deploymentist, ma juba näitan see laseriga. Ja deploymenti asemel me võime ülesseada ka, kas stateful seti või demon seti. Deploymentis on näiteks kolm poodi, replike seti tõttu, siis ne poodid saadud sellised suvaliselt nimedest, random, genereridud nimedet näiteks. Peale deploymenti poodid võib-aad olla nimetatud näiteks peale, siis deploymenti nimi ja siis mingisuu random ID sellele kootide näiteks 237, 6, teeme nüüd elist. Kui me seame üles stateful seti deploymenti asemel, siis pannakse nimet staatiliselt nimedet esimene poodide, et peale 1, teene poodide peale 2, 10 poodide peale 10. Stateful seti on kasutatakse armebaasi tüübi rakkendustana, kus on vajalik, et äitsid nii täpselt kontrollideks, et kui me seame üles ühe nüüd ja armebaasi nüüd, siis tema on näiteks liiged ja tema on vastutavse selle clustervii oksutamiseest. Kõik järgnevad, näiteks, poskres 2, poskres 3, poskres 4, poskres 5, nema ei ole, siis liiterid nema on näiteks replika, nema veel liiterid nema on võitsu replika nodeid. Reed replica nodeid näiteks. Stateful set on sarane nagu deploymenti, nagu mõõdud, kige on staatiliseks staatiliste poodid ülesseadnud, et poodid nimed on tähtsad, et ei oleks random nimed. Demonset on ka võimult, sest te kui poodid sarane, et jääb üles poodid, aga siis me ei kontrolli enam nimekal nende poodid arvu, vaid me soovime seada poodi iga kuperneetises nüüdibel üles, iga serveri peal. Et näiteks, kui me soovime kuperneetises serveri peal hakata koguma logisid, me võime sinna panna näiteks logi scraperi, ja me tahame, et igas kuperneetises nüüdibel jooksas logi scraper, siis me paneme sinna üles konteineri, Demonset kontrollib, et pood panaks üles iga noodi peal, ja sinna maundime sellesa poodi logi kaustad väljespool, ja siis see pood hakkab tegelema nagu logifailide söömise ja kuskile kesksesse logi koidlase saatmusega näiteks. Demonsetiga saab siis panna näiteks, kas nüüd eksportarid või mingi taisad tööva iga kuperneetises nüüdibel. Lisaks on ka sellised grand job tüutki molemit, mis tegitevad vastavalt ajastusole töö, et iga öösel keel kolm tegiteks kuperneetises töö, ja kuperneetises tööobjekt kirjadab, et mis konteineri peaks ülessead, mõe mis poodi peaks ülesseadma, ja tööde esmerga on jooksutada töö, kuni see väljastab, et õnestus. Ega grand job paned näiteks, iga öösel jobi tööle, ja job paned poodi toole ja kontrollib, et see pood tagastab nagu saksessi põnima, et ei oleks errolo koodi. Ja siis pood jooksev ja kui pood väljub korreksata ei tagasti errolo koodi, siis job on nagu tehtud, et grand job siis kontrollib, kunagi paned selle job tööle ja job kontrollib, et käilitada töös, et jooksalt korreksad. Aga poodises võib oleks ka kahtu tüutki konteinerid. Et meil on tavaselt konteineri näiteks Postgres konteinerid, mille esmalt on jooksutada Annebaasi processi, aga meil on eraldiga inid konteinerid, mis tavaselt peavad jooksma enne, kui see peakonteiner jooksad. Ja need võib oleks valmistada ette, millid kaustad, millid failid kaustada ja failid õigused, mingiselt konfiguratsiooni failid ettevalmistada. Ja ideea on, et selleks, et käilitada peakonteinerid, peavad kõik defineeritud inid konteinerid enne ära jooksma ja kõik peavad Anneba takavast samultis saksessfull koodi. Et kui inid konteineri annab eroli takasi, et tal näiteks ei õnestud mingit faili õigusi muuta, siis ei käilitada nagu tavakonteinerid. Et näiteks deployment-sest defineerid me tahame jooksutada, ötselt et Stateful Set defineerid me tahame jooksutada kolgun. Annebaasid pood hakkab jooksma, aga enne jooksutada inid konteinerid. Ja kui inid konteinerid ei takastada saksessfullid, siis väriks konteinerid jooksma ei panda. Ja kui pood saabsid lähebks konfiguratsioon, et viis korda inid konteinerid jooksutamisel vead, siis Stateful Set rääst deleitet selle poodi ja panu uuesti poodi tööle näite Stateful Set peal. Ja siis käibid, et see uuesti inid konteinerid ei korda tuni nagu kõik takastavad, et saksessfulli. Ja jälle siis panaks see päris konteinerid jooksma. Ja et selle apina võimalik automatiseerida, mis viisil me konteinerid ülesseame, kas me seadistame nad grondsoobid enam. Ja ka see, mida me teeme selleks, et konteinerid protsessi ette valmistada enne, kui ta jooksma panaks. Nii et sellist asja võib me ikka automatiseerida. Kui Tokers on lihtsalt, et kas Tokers service või Toker Nodes, Toker Runs, siis kui pe need, sest me põimsed defineerime, et mis jüüddi konteinerid ja töid me tahame, ja siis kui pe need esi ise orkestreerib seda, et kuidas jooksutada ja kuidas kontrollida, kas asja jooksad projektsi. Ja teine tähtis olema on siis TENUS, et ütleb, et me olema oma mikrotenusest appist pannud deploymenti üles. See deploymentis on epikes, et mis jüüdda, et me ei appistav jooksma kolm koop, ja tavas me seadistame, kui pe need esimene asjame nene poodid, et me ei rakenda sana, et põid poodid saavad rakendas liikvallis ala sellised lippud või leipolid. Ja kui me nüüd tahame liikvlusi takka suunama nendele poodid, et kuias, mis aadresid tulevad liikvlusi, suunab nendele poodid, et me saame teha TENUS, meid on TENUS nimeks, nagu aapana A, ja TENUS, et kasutavad sellist neid leipolid te mäksimist, et TENUS, et sa kirjutad, et otsid üles kõik poodid, millel on selline leipul, ja me ei kasutada liikvlus edasi suunamuseks. Kui TENUS, et see tuleb, ja TENUS on oma impaadres ja kostmeid, et kui TENUS, et saadad et kõik mis liikvlus, siis TENUS, et otsidaks selle label matchingse poe, et mis seal hetkel jooksvad koodid, millel on see rip või see label, me suuname sinna ümber selle liikvluse, ja seal tohaimub näiteks Round Robin liikvluse suunamined ja LooT talentsemined kormu sealturet, mis neide korme containeruonele, millel on täpselt sama liip, mida see, või label, mida see TENUS otsidaks saadad, see liikvluse tünda. Ja kriavase taosta toimuks liikvlusest suunamad selletipele adresid ühele Render, kallrastiipele adreside, ja kogu võrgul liikvluse tasemel on täpselt, nagu Linux'is rootims reegid, et mis IP-le suunata, aga nagu loogiliselt tasemet kasutab, et sellist label mäksid. Ja miks see on hea, et ütleme, et näiteks me tahame seda pootide versiooni muutelt, neid on meil versioon kaks. Ja kui me tahame seda pooti edasenduta uue versioniga, siis me saame meil meil eemaadale liipu, kui seda liip pärame jälle siis siia enam liikvlusti tuleb. Ja me ootame, et me saame üle samal teise pooti uue versioniga, ja kui siia enam liipist ei ole, siis me kustultame selle pooti ja parem uuele pootile siis sama liip. Ja selle tulemus on hakkad, et see suunamad liikvlust uuele pootile, millel on sama liip edasendu. Me saame, et lippe kasutad selleks ajutuselt näiteks liikluseste eemaldada mingisugune poode, et tema enam sisse tula päringub ei saa. Ja samastime uue kontene, et me saame lippu sellele panna ja siis ta hakkab liipnust saama. Me saame selliseid label, et kasutab ja potstustub, aga kas see teenud siis saadab praegu liip sellele pootile või mitte. Ja me ei pea nagu madalalt asemel kustultama asju, vaid me saame lippe eemaldada ja lippe panna, et osustuda, kas hetkel liiklus sinna saada või mitte ja see aitab nagu sellised dünameiliselt asju ümpe konfigureerida. Ja isegi, kui meil on näiteks sellest deploymentist väljas pool mõnii poot, et ta on kaas sama liiklus. Sildi te või defineerida näiteks siis, kas rakkandus on nime, rakkanduse pootid eril version, et veel võib oleid oleid lippud a.1 ja a.2 näiteks või siis eradi lipp. App võidu paha ja siis versio on võidu piikst ma nuli. Me saame ka teenuses kasutada näiteks versiooni labelid selleks, et asjuks või kas mõni lippus saada. Või siis saame näiteks teha äraulti need labelid näiteks test või keskonna, test keskonna või arenduskeskonna või production keskonna jaoks. Teadult teenuses suunavad liiklusa test keskonda ja teadult teenuses suunavad production keskonda ja siis testimiseks production keskonna saadmiseks ja kasutam teist teenust, kui test keskondi vaatnud. Ja see on kõvasti, ma ei teinud siis kõvasti, aga see on rohkem nagu sellised teenuse objekte ka. Meie praegu rääkisime sellest, et kuidas me saame kasutada teenusele poodidele liiklus suunata ja poodine on oma IP-aavrist ja ospeimid, servisidele on oma ospeimid ja IP-aavrist. Kui me taham, et väiaskot, kui pärjad see kasvõit võrk puuliiklus saada teenusele, me saame kasutada ingressi ja ingressi saame definierele tomeeni nime. Kui meie klastriadres on Pelle Cluster, siis saame definierele ingressi, mis on app1.pellecluster ja see on ospeimid, siis ingressi ja ospeimid. Me saame defineerida, et kui väljas tulevad liiklus sellale inters IP-aavrissile ja poordile, et vastavad ringisugustele, näiteks Satomai nimele, mille sa näite app1, vastavad sellele suunata liiklus servisile, mille nii on app1 ja siis servisist suunata liiklus vastavad labelile, kus mille väärts on app1 ja niimoodi saame definierele ümda suunavast. Ingress on selline väline kontroller, tüsti kasutab, et näiteks nginx ingress kontrollerid, et siiahtaks üks nginx kontainer iljast, kui päris edes ja teaks selle üks port, millega auto tema välis liitust kuulab ja siis selle index kontrolleris otsustab, kas liitust edasi saada sissepoole või nitte, et on ka selline, kas ta just tulemüür on, aga põhimise, et võtab sellise tulemüürin rooli, et otsustab, mis liitust siis sissepoole saada. Ja netwerpootidele võib ka annab panna ka netwerpolis, mis defineerib siis, et kui poot soov kuhugi jäänust teha, mis suurkustesse namespaceidesse või mis suurkustesse pootidesse tal on lubatud liiklus saada. Ja servisid võib kohele erinele tüütbi, kas ta on cluster IP vai node port tüütbi cluster IP teha, aga cluster IP teha, et ta on selline IP address, et kui mõetest, et see kastris ja selgselt sinna, et see teinusus liiklus saada saada sellel IP address või teinuse nimele, liiklus, kui teinuse nime ise annab ostmeid. Aga node port on atküldi teist suure teinus, et selle aseme, et meil on unikaali IP sellel ja teinusele, et klostrii kui mõetest serveri IP address ja mingisugune lokaalne portnäid 8080 või 5000 või 22700, et tekib mingisena port, kuhu me saame üks tõik mis kui mõetest sen uutida liiklus saada, et me selle ea teadma teeluse IP addressi või me teame, et on suvaalise kui mõetest serveri IP address ja mingisugune fixeerid portnäid 8080. Erad, kui me oleme väliselt teinus, aga me ei teast viimel load balancer, mingi pilve DNS, et me siis ei otsustada, mis on selle teinusele adress klostrii sisemisel, vaid, et küsime teinusile mingi IP address, mis on Amazoni mingisugune välisest load balancerist. Kui liikus läheb väliselt load balanceri address, siis suunadaks ümberda kui pärneetese sisse. Selleks, et me saaksime neid pood skaleerida, on meil tegelikult aesti lihtne, et me defineerime deploymenti CS replica seti, kus defaultina võib-olla, et meil on üks pood, kes mingil hetkel muudatakse ümber selle deploymenti konfiguratsiooni failis, jamblis võib tõhimist alvepaisest appi kaudu, et nüüd replikate arv on 1.4 ja see on kõik, mida ta tegema, võib-olla lihtnata appi, et ära muutma, mis on replika seti, replika vähestas ja kõik. Taustal, kui pa näete, siis ja olidseb selled, kus johdata seda kolmne omuute konteinerit ja kui ta see meid keema panna ja mitu korraga keema panne nii edas, et toimesed me ise jooksuta otsaselt käsku, vaid me pigem ütlema appi kaudu, et teaks nüüd olema rohkem poode. Ja tegelikult on tähtis, et oleks rohkem võibks replika tavaliselt, sest kui üks konteiner kokku jooksad mingil põhesel, siis liiklust ei saa teistevaale ära jagada. Aga samas on teil lubatud teeskaleerida 0-li, et ei pea isegi deployment ära kustutama, et ta võite replika setist seada pooditarvu 0-lit, siis kustutu olid 3 pooditarva aga see deployment ei pallas. Ja kui keegi nüüd muudab selle replika arvu 0-list ühen, siis selle panaks esimene poot tööle, et kui kubernetisest seest saab misselt skaleerida 0-lit, et kes on tööd, et näiteks, et meil ei ole vaja jooksutada mingid ofis rakkensest, kui kedagi tuleb jame, et siis saame konteinerid võib poodnud misselt skaleerida 0-lit. Ja tänu sellisele automatiseeriliselle on tegelikult võimalik kubernetises hästi palju automatiseerida. Me vaatame ühtel näidet, et kuidas me saaksime näiteks kas uuendada nende konteineride versioni, ja ka testida, et kui me paneme nüüd, näiteks meil jooksest praegu, et replikade tõlseli poodi, siis see on ühtel poodi, siis nüüd me näiteks ta, et üleks seada, et see on kahe, ja kudas siis replikade muutmine töötab niimoodi pitte replikat, 4 replikat, siis välja vahetud suute versionitega. Ja me saame sisse eitada testimise, et kuidas testide, kes uus version töötab korrektioll. Selleks siis teerivad erinevad juurutustrateegiat, kus osa nendest on automaatselt implementeeritud kubernetises ees, aga osa pigem nõuavad ka natuke rohkem ülesseadmist ja automatiseerimist, et ei kruugi need viimased olla automaatselt rakendatavad, kuna tuleb kombineeridega erinevate uute teenuste loomusega. Ja ideo on, et siis, kui me tahame tegime tokkeri containeri ümber, või tahame nüüd uue teke poodi, siis et siin laime rekistes uue containeri versiooni, ja näiteks me kirutame siis poodi definitsiooni ja selles deploymentis, et nüüd on meie container imagei versioon teise labeliga, et me näites uuen, et on lihtsalt ühe containeri versioon. Ja mida kubernetis siis teed? Taustal kubernetist hakkab kustutama poode. Ja see on hästi tähkist, sest suvalisal ajal võib kubernetise otsustuda suvalist poodi kustut. Nende kunagi saab kindel olla, et kubernetise kustutat teie poode ära. Ja kubernetis ei ole sellist asi nagu containeri sisemise processi restarti, nagu tokkere starti, põhimesed kubernetis ei ole. Et ainuke viiskudas millegi restartida on pooteliid. Ja kubernetis ongi, et see pooteliid võib juhtunud suvalisal ajal, näiteks siis, kui containeri tarp vara uue endatakse, või siis, kui on ülesseatud niiugusugune automaattik, et kubernetis hakkab loolutal lähe sema neid serverid. Siin on liiga palju hästi jooksma, et ta tahab, et siin serverids oleks vähe poode, siis ta kustutab selle ära ja liidutab teid selle kundmuna. Veid sa suvalisal ettele võtta jäägi ühe Postgres klastri poodi ära kustuta. Ja sellest ei tohiks, kui kubernetis siis midagi nagu alvasti juhtuda. See poodi kust teema ei ta ole võirega poodi üle. On, et sa võid näiteks hakkata defineerima sellised asju, et sul on meid prioriteedid. Sa saad defineerida prioriteetene, et eks meie sursside, liniitide kautta, et seda me selles ainees ei katta. Ja te võite ka nagu põhimõtteliselt teha niimoodi, et process iseg kuulab signaali, et panakse, et seda ei teha nagu hard killi, vai teaks ikkagi saad, et see normaalne kill signaal, ja process võib saan isekinnipüüde asjad ära teha ja siis normaalselt väljuda. Aga põhimõtteliselt suvalne tarkkora, mida ehitataks, peab olema selline, et ta on suvalisele kustutatav. Ja meid konteinerid ja tarkkora tuleb niimoodi arendada, et ka anmebaasid on täiesti suvalisele etke kustutatav. Te saate igasugust muid asju teha, et on see toleransid ka, et näiteks, et ta tolereerid erinevaid signaale asju, et teda ei oleks nii lihtne kustutada, aga põhimõtteliselt kuberneetes on ikkagi desainitud niimoodi, et suvalisele etkel võib kuberneetise kontrollereid asju kustutada. Ja selled oto anmebaasid peavad kasutama sellist asju nagu right ahead log, et nad suvalise operatsioone enne täitmist kirutavad logisse, ja siis ales hakkavad täitma, kui keegi pilib neid, see on täitsa okei, kui sa kimmi panaks, ja see right ahead logi on kusakil persistent storage's, näiteks, file systeemi kirutatad, järgmine korv, kui sa sama pood käeviitub, siis ta vaatab logist ja vaatab, et mis sellist näit operatsioonid, mis ei ole veel täidetud ja teed need uuesti läbi nõudlist. Pige mõn, see ka muud kogu see dockerit ja konteeleerit ja kui peab näet, sa areng on ka tegelikult muutnud, et mida anmebaasid tarp pala peab tegema. Selle tõpelt näiteks igaslused rabit ja kafkasannased message brokerid ka kasutad right ahead logi, anmebaasid nagu poskas kasutad right ahead logi, selleks just, et väestida olukordad, midagi läheb kaduma kui keegi protsis järgmine peab pala. Kunagi tead, et jäsku serverist teha kui restart ja serverist panas kinni ja keegi ei saa ehelda, et anmebaasi protsis ei tohi suvalise ettei kindi panna. Nii tõrked aluse, kui ka kubernetse jahks tegelikult on vajalik, et suvalise ettei võib saada põhensid kindisignaaliprotsis. Ja vaatamegi neid juurutustrategiaid. Ästi lihtsalt re-create, mida te saad ise teha, aga mis ei ole default tegelikult. Ramps uueldamistrategia, mis on default kubernetses ja siis selliseid logilisvii uueldamistrategia nagu blue-green, canary, AB testimine ja shadow. Re-create on see, et te saad teise, teil on rohelnõnese vana versioon, versioon 1 ja ise te liidite teise deploymenti, paate kinni ja siis lasete uue deploymenti tööle, et teete käsid sinna ku versioonu uueldamistrategia. Seda kubernetses ei kasutata, aga te saaksid seda administraatore ja käsid teha, et ma ajateleiselt vana versiooni deploymenti kinni või skeelite nulli ja siis paate uue tööle ja jääb natka aega, kui teie rakadist tegelikult ei ole kasutatud, kui need uue poodid tööle jäävad. Seda tehtid jaoks näite stokkeris. Paletake mingi tokker teenuse kinni ja siis paate uue tokker teemist käima. Seda on kasulik teha, siis, kui resursid on hästi piiratud, kui on vähe aktiivset kasutajad, kui teieoks ei ole tähtis, et kõik kasutajad saaksud see rakendist kasutatud vahepeal ja üldises teist kulude vähendamiseks, et teil ei ole vajalik lisa resursse, et näiteks kahte versiooni sama äksut rohsutada. Või tuleb kui ta kui ta kohde, et kahti versiooni korraga ei kasutata, et siis Anne Paaside pool, et teil teab siin vahepealt olema näites Anne Paasi skeemade uendus, et te paate Anne Paasi kinni, konnetereid tege skeemad ümber ja siis paate uue versiooni tööle ja vahepeal ei tohi toimuda kirjutamist, mis kasutab vana sellist skeema. SQL Anne Paaside pool, see tavaliselt probleem ei ole, sest kui skeema on vale, siis kirjutamine ei õnestu, aga teatud mitte releatsioonist Anne Paaside pool, lubatakse Anne saata suvalise skeemaga ja automaatselt uuan saab sähkest tulpased ja Anne Paasi võib näpat sega soks minna kui indegi kasutatakse samal aal kaks periaal versiooni, mis erinele viisil Anne et Anne Paaside alestadud. Näiteks inflaksis või kestigis juhtub see, et on lubatud näiteks tabeli automaatne geneerimene ja kustutate vanatabelistruktuuri, järetood uue tabelistruktuuri, kui samal aal pärast seda kegi Anne sisestab, siis lueks see vanatabel tagas. Nii on kui tegad, et mõnikord on hea väestpidat kahtereendverisooni samakastud. Aga see ei ole sellega default uuendavistrateega kui päris näiteks. Default uuendavisestrateega on pigem selline, et te lioks näiteks 10 konteehnerid see vanas versioonis, ja kui te siin muudab ära näiteks deploymentiselt, mis on uue versiooni, mingi tokke vinnis, siis mida, kui päris näiteks tegemakab, ta hakkab ühe või rohkema konteehneri kropa lihtsalt välja vahekama poode, niimoodi, et näiteks, mõne on 10 pood jooksmas, esim alvu teed uuest versioonist uue koop, et mingin hetki jooksad näiteks 11 konteehnerid sama aaksad, ja siis, kui see uus pood töötab korreksad ja on valmis vastu võtma linklust, siis hakkateks sa manuul kusutama. Ja tehaaks nagu rolling cocktail, niimoodi, et näiteks iga 200 asemtab, et siis üks pood ära, siis jooksvalt, vanas versioon, kui enam vanas versiooni pood ei ole, siis see on kui luid ja kõik ära asemtab. See vajab pähamud iht ja lisab poodi, et ma tegelikult ei eemalt ülti liht poodi enne ka muua lisame, nii et see nakkutkene vajab lisada susse, ja kui neid pood on palju, neid on 100 replikat, siis tõenest me üheaval neid vahetaväljad, siis neid on defineerid, et vahetatakse neid 5 korraga välja. Niimoodi, mis on seal hea, on tegelikult poodide template, on võimalik sisse kirjutud automaatkontrollid, et kas pood on valmis liiklust vastu võtma, see on nii laivlines, kui readiness checkid, et me võime näiteks defineerid, et saada minu poodi sellel kordil ketpärindiumise ketpärindiumise kood on 201, siis pood on valmis liiklust vastu võtma, et saab automatiseerid kontrollid ja checkid, et kuna liiklust poodi saata ja kas pood tagastab korrektse kodi ja kui tagastab korrektse kodis järe, kas on uus versioon töötab. See on siis defaulti käitumine, et kui me ningis kusest deploymentis, mis on palju replikaid, nagu poodi definitsiooni muudame, siis ei juhtu see, et korraga või kindi panaks, vaid see koimalt aegmasal üle aja ja see tavasalt toimab ma ühe poodi kaupa, aga me võime definerida näiteks 5% kaupo või 10% kaupo või siis kumbo minimum kas 1 või 10% näiteks, et kui siis on 100 replikaid, siis hakkab see 10% välja vaatama. Rollback on võimeks, see on kaks asja, esiteks sul on võimalik paus teha, et sa võid siin näiteks öelda, teha paus, eest ta jääb selle ta sama ja siis sa võid näiteks huurida, okei, siis kui teadab hästi, siis järgata sellega või siis sa võid öelda rollback ja siis toivuks vastu mitte. See on etteks siin aga teist testustama. Et ta võimeks, et järgtaab meelde, mis oli eeline version sellest deploymentist Anna Baasi ja see ise jääb saad lihtsalt rollbacki teha. Eriti kui sa märkad, et logins hakkama terrorid ningi loginsistenei hakkama terrorid tegema, sa saad lihtsalt rollbacki teha. Ja rollback teadab siis vastukidisel viisilaks samamoodi, et ta on tööda paegu tead. Ja siis täiendam saav, siis need valat poodid esit järgavad liikluse vastamist päringite vastamist. See võibist teadab niimoodi, et kui me on nelil replikat ja me soovime see uuededa, siis kui perneetes loob ühe juude kusagil kontrollib, kas see uus uus rakendis on valmi võtma liiklust ja kas ta on elus, kas ta vastab korrektud ja kui on, siis eemad võtsevad maha. Ja siis üks aaval järjest hakkab et see on eemadava, kui kõik on välja maha. Ja põhjeliselt sisemist, mis juhtub, et vanadur poodid on üks IP-aadres, et 05.06 ja sinna on 1.5.016, et lihtsalt labelid muletab. Meil on selleg versiooni label ja me ootame, kuni see pood on valmis liiklus vastavõtma, siis paneme tälle label ja see mapele sit labelid ja põhjeliselt seda võib-pal natku ootame, kuni enam sinna liiklus ühtegi aktiiselt sessioni alle, siis panas tagim. Me võime ka teha prune-green testi, et põhjeliselt me paneme, me ei jookse panaversioon ja mis me teeme, me paneme tööle korra kõik uue versiooni poodid, et meil on näiteks 20 panaversiooni poodi ja me paname üles 20 uue versiooni poodi ja minges hetkel, minges sekundid siin switchime lihtsalt üle. Nüüd service enam ei, näiteks meil on siin vanal versioon, ei ole label 1.0, uuele versioon ei ole label 2.0 ja siin selle hetke me teelasel muudav lümbere, et teenus enam ei suuna label versioon 1.0 ja vaid label versioon 2.0. Ja nagu hetkega suunates kui uus liiklus kohe uuele üle ja meil ei ole tegi seda olukord, et samal ajal kasutab see kahkere nüüd versioni, aga meil on vajadus, et meil sama ajal sellel hetkel oleks kaks komplekki kõikides poodides juoksmaas, et me vajame ka juba kaks korda rohkem tege sussa ja mingi ajal. Ja me võime samult nii nüüd logides jängid, et kes uus versioon on takkama, või saame poodid switchi teha vanal peale tagasi, et me saame vanal poodid jooksma jätta ja me saame instant switchi uuele versioonide ja instant switchi vanal versioone tagasi teha. Et selle puudus elmsel onki, et see üle minne kuule versiooneid ajal, kui ta siin võib-al töötab 20-12 minutid või ta vaa aega, siis selles AB-blue-green strateges toimub kohesalt see üle minne, aga rollback toimub kohesalt. Muudiks rollback ei toime kohesalt, kui me need vanad poodid kinni paneme, aga meil võib olla mingi aaja hakkean, kui me ei oksutama neid kahti versiooni sama aeksalt ja saame. Ja selle eelis on siis siis, et me saame kohe, kui erroreid märkame tagasi jüpata, aga puudus on siia, et meil vaja nii ka aes kaks korda rohkem poode ei oksutada. Ja... Aga jah, see läheb siis kallimeks. Ah, mul on siin väga hästi nagu ülegi tõmpsnud, aga see ongi. Teine on siis kanarid. Idee on siis see, mida sa isega mainisid, et me teeme mingi hetkel pausi ja me kasutame sest rampstaateeget, et me hakkame siis uut versiooni lisama, aga me tahame, et mingi on jälgide, kas need uus versioon tõedab korreksata mitte. Me juba defineerime, et me aselda ma ainult 10% vanadest ära ja jälgime näiteks mingisugust 10-15 minutid, kas tekivad logidis erroreid. Ja kui logidis erroreid te tekis, siis me kasutame järsku uuendust, et üleminna uuele versioon. Siin on siis heitavad jännilikivist period, mis oleks piisavalt pikkeb. Ne teame, et me logidest läia me veada üles, kui sa uus versioon millegi pead pakka ma ei saa, või siis läiteks mingi klientid elistavad meile, et kuurge märk midagi ei tööta. Siin on selline paus pohjumist siis heitavad. Ja kasutatel ei prugigi märgad, et osa on edasi kasutavad malam versioon, osa on kasutavad uut versiooni, aga seda pigem ei tea, et me näiteks 5-10% kasutavadest kasutamist testi ja ta näad. Ma aata, mis logidis erroreid tekivad, kui nema kasutavad seda. Ma asun siis testin productionest. Jah, aga see on ajukkine. Ja küsivam testin production on siis AB-testimine, kus me põhimõtteselt teeme kaks erineelt frontendi või kaks erineelt back-endi. Me lasume osadel kasutatel kasutavad ühtemersiooni ja osaltatel kasutavad testversiooni ja siis me võrdleme näid kasutatel tagasusid, või kasutatel mugavust või mingisugud meetrikad, et näites, ku palju nad plikke teemad, ku palju nad ostavad, et jäsku versioonis A vastatakse rohkem tooted, ku versioonis B, et me sama aaksed jooksutame erineelt versiooni rakendest ja vaatame, et kumme veel jääb, et see on nagu proovid, et me proovime mitut erineelt versioonid back-endist ja frontendist. Tihedamine täeks seda frontend rakendustega, test mobiiliv rakendustega või poer rakendustega. Ja näiteks mitut erineelt algoritme kasutama, et kudas näidata kasutatel, et millised tooted näidata sorterimiselle esimesena ja needas. Me lasame erineelt kasutatel, kasutame erineelt versioonid suurimegi, kum paremne töötab ja kum näidast võidab, siis hiljem seia põlgub. Ja siis me saame lihtsalt kaava erineelt pootile kasutame erineelt leibaleid ja saame kasutama loogika, et teatud liiklus suunad sühteteenus, ja siis niimoodi otsustatab nendeks, kui kasutatel kui kliendi ID põhjal või otsustata kumva versioonid kasutatel. Aga nii on tähtis, et ei tekik seda olugu, et samat kasutatel suunata erineelse versioonid, et me tööme karateerimelt. Üks kasutakindast näeb ainult seda versioonid, keda ei visata erineelse versioonid, sest muidu nende logid ei näita seda, kum neid rohkem meeldib. Et me pigem aga me kasutame nende versioonid omaid arv. Kus siin see ei prugi olla vajalik, võib-olla meil on okei, kui kasutatel näevad erineelt versioon, aga siin on meil hästi tähtis, AB testimise hästi tähtis, et sama kasutame aalt samal version. Lisaks on ka sene Shadow testimise version, et see on live testimise tegemine niimoodi, et kasutat midagi midagi näe. Me tahame teha 100% live testi, et paname uue versioone üles, ja me tahame täpselt võrra, et kui hästi uus versioon võrdle panaversiooniga, ja meil on täiks okei, kui tulad erorid, aga me tahame, et kasutat kunagi eroride näevad. Mis me teeme, me seame üles kaks versiooni, paraleliselt, täpselt samal tajad poodid arvuga, ja meil suuname mõlemasse kasutajaliklusel. Kasutajal teavad päringud, need päringud läheb mõlemase. Aga ainult originaalses versioonis, et see on üks vastus, et tulevad tagasi kasutatela. Versioon kahest me ei suunad requestid, et vastusid tagasi originaalset kusutatel, või me põhimest kustatame need responsuid ära, aga logides vaatame, et kum versioon parem ei töötu. Ne põhimest, et splitime ja repliceerime sisse tulevad päringud, aga me kasutame seda teist komplekti poodidest ainult testimise aks ja põhimest võrdleme logides näiteks, kum nendest on jõudlik, kum kasutab pähem mälu, kum kasutab pähem seet, et jõudin edasi. Me kasutame nagu testimise. Et see läheb ka tohutud kalliks, sest me teame mingi aegi oksutama kaks korda rohkem resursse ja kaks korda rohkem liiklust ära töötlema. Et see läheb isegi rohkem kallimaks, sest liiklus ka tegelikult laseb selle jõudluse. Sisse tulevad päringud ja töötluse teame kahe kordsed siis tegema. Et mitte, et meil ei ole kaks kord rohkem mälu, aga kaks kord rohkem seet uudle. Ja kasutad siis avad endiselt vastuda ainult, kõige vanemast versionist ja me hiljamse switchime ka suuele või kusutama uued teist ära. Kui panneet, et klaster ise koostakse kahte tüüppi serverid, et tavaliselt kui te ise seate selle üles, siis te võib-alla seate üles ainult ühe tüüppi serveri või ainult ühe serveri. Aga kui te soovik nagu hea klasterid, sest tuleb luua sellist tööliste sõlmed, swirkerid ja juhtid teenustest sõlmed, mis ainult tegelevad nagu klasteri juhtimisega. Sest kui te miksite neid, et panete samal ajal tööle, näetis mingisugused annebaasid ja mingisugused kontrollerteenused saman uudid peal, siis lihti võivad probleemitekid ja meil Kubernetes-signals näetis tekisid. Tutengid tapsid oma klasteri maha sellega, et paneid üles tarkvar, mis kasutas liiga palju mälu ja liigavalt share-brewed, mis tapis pohjamesse, tapiserveri, poodid ära ja kastere on kasutatud. Klastri juhtimiseks kasutukse tõimised haldusteenuseid, mis iie oksavad konteenette sees. See on hästi tavalne mustper, et te seate üles Kubernetes-klastri ja mida iganest te soovite, see seate üles Kubernetes-sees. Soovite annebaasideenuste Kubernetes-sees, soovite persistent file storage, haldustarkvara, seate üles Kubernetes-sees. Soovite uud Kubernetes-võrkulahendust, see on vastupidine pilvene, kus tead pilves, see on oma rakentuse ja või toalmasendu. Konteenereid, mis kasutatada mingil välised pilved eenuseid, et näiteks files alvestada võrku ümber konfi või VPNi teha. Aga Kubernetes-seest te seate üles kogu kestuna ise Kubernetes-sees. Ja see natukena haitab nagu sellel lukustamise, vendor lukustamise vastu, et saate suurstseelt hästi, mis iganest te Kubernetes-sees üles seate, tõstada üles teise pilve oma serveretele või oma serveretele pilve, sest see keskummel täpselt sama. Ja kasutakse sama Kubernetes-versioonid piktoed kus aavadi ja te väga ei kasutagi väliseid teenuseid. Siele on tegelikult agad, et te saate pilved eenuseid samulti kasuta, aga te ei pea. Ja te saate asjad seada üles väikse serveres ka, et Kubernetes-seest on versionid, mis väga palju resursse väa, aga tihtan idee ikkagi, et te seate üles suurstseelt suunta serveredest ja kasutate mingil kolme node või rohke node. Ja Kubernetes-seest juhutasandu komponereid, mis samundi töötavad kontainereid ühe. Kubernetes-seest näed, Kubernetes Appi Server on kontainerid, Kubernetes Scheduler on kontainerid, sissehäminne hajusandu või kontainerid ja eline kontroller mene, et see on kaal töötavad kontainerid enam. Ja tööliste selline testavad töötavad ainult, näites kontainerid runtime, mis on siis kontainer D või Docker või midagi mood. Ja siis kublet, mis kasutab kontainer runtime-i, et jooksutada kontainereid ja tähenglikub perenets Appiga ja siis GameProximise nagu võrgu ümper suunamise ja võrgu Firewall tarkvara ümper konfigureerimise teenus. Ja tööliste selmedes nagu rostkal tegelikult palju olagi ülendaselt töötavad kontainerid enam. Et tööliselt siis töölistisse servered siis jooksutad kontainerid ja hoolitsodad, kuidas liiptus nende kontainerite nii ualt. Ja need kompanedid andju kõige keskkone Appi server, on kontainerid kõik käis läbi Appi serverete kaudu. Ja Appi server on ainult, kelle on lubatud annebase tehti kirutada. Ja annebase soitakse kõik, kui perenetsse liiseltad anneid, nii mis on hetke staatus, kui seda, mis on soovitest staatus. Et kui kasutaja kasutab Appi serveri, koha sa ütleb, et loob on uus deployment, siis Appi server ei jooksuta käst, kui ei jooksuta käst, kui ei jooksuta mingi uus kontainerid. Vaid ja kirutab lihtsalt annebase, et nüüd peaks eksisteerim uus deployment nende konvekurentsioonidega. Ja kui nüüd on vajas, et kontainerid või poodelid jooksutada, siis eksisteerim scheduler, kes kuulav Appi serverit ja teed pärilud. Ja tema saab teada, et nüüd onnud suur deployment jooksutada. Tema otsustab, kus hakkab ka poode jooksutama ja kirutab Appi, et nüüd peaks eksisteerima need poodid, näiteks nende nootid peab. Ja ka scheduler ei mõta ühenest otsa võrkeridega, et jooksuta midagi minu heaaks. Vaid scheduler lihtsalt ütleb Appi, et kirutab annebase ja nüüd poodid peaks eksisteerima ja mingisuguse võrkeri pealt öörtab kuublet, ja ka kuulav annebase saab teada, et nüüd selle serveri peab jooksuma kolm pood. Ja tema vaatab, et mis on näide poodid templateid, ja siis ise otsustab kasutavad näid stalkerit või kontainerid tee, selle serveri peab panat tööle, siis kontainerid mis sa paegamme. Kõik suhtlevad Appi kaudu, kui pärineetesega, kui pärineetes Appi hoiab annebases neid anneid, ja see on haius annebase, et iga nõud saab läbi Appi, nagu infotsit. Haius annebase, aga kõikeb Appi kaudu, nii et kui Appi kontaineri mahaläts, mitte ei teha sa. See ongi nagu selline huvita olukord, kus Appi kontainerid, et ta on veel mitu replikata, ja keegi näiteks valesti konfigureerid Appi serveri kontainerispetsifikatsioonis, midagi panat palje versioonis, siis kogu kuba näid, seal on otsustatud kasutata läbi Appi. Existeerib kui pärineetes admin käsut, milleks saab olo korda parandada, ja on võimalik, et näiteks mina serverisse, kus Appi kontainer juoks, ja tockeriselt kontainerid ei aab nii midagi imbe muuta, aga võib-lihtsalt on olo kord selline, et Appi kaad elu jääma, Appi kontainerid kaad elu jääma, selleks, et kogu kluster hästi töötab. Ma juba mõned asjad läksin ette, aga ongi Appi serveris, võtab vastus Appi käske, ja mõnesvõttes töötab kui frontend teenusena, et kõik suhtevad tema ka, kui midagi frontend joksulust ei pakku, et ei ole aastikelt rakkandus. Appi serverid alati seeteks üles mitu koopjad, ei ole hea, kui sellest on ainult üks koopja, aga kui teine on ainult üks nud, siis ei ole võimalik see, et see on mitu koopja joksnuda. Ja kui Appi on maas, siis peab Appi kontaineri uuesti ülesseadma, ja siis saab se kuba näid saab uuesti kasutatud. Ja kõik sisse tulad kuba näid saapi päringud jagad, kes on nende kontainerid vahel ära, et existerib jälle üks kuba näid tese teenus, mis on kuba näid saapi teenus, mis suunab kõik sisse tuuad liiklused nende kuba näid saapi kontainerid vahel ära. Kuba näid saapi ise on kuba näid tese sees jookse kontaineride teenuse. ETCD on siis hajus annebaas, et kui Linux on ETC kaust, mida agal onid siis minuks on kajutatud just konfiguratsiooni failide hoidmiseks, siis ETCD on see lihtne hajus Linux annebaas, kus iga kontrolernoti peale joksab koopja sellest ETCD annebaasist, ja kuba näid saapi kasutab seda, et hoida siis hajusad konfiguratsiooni. Ja kõik üks kõik konfiguratsiooni on, et kui teed on kõik konfiguratsiooni, ja üks kõik, mis teie kirutad appide, et näiteks midagi teha ümber muuta, uus poor peaks olema, siis kõik kõigevst kirutus annebaas, ja siis ilm teised procesid saad annebaasi kaut teada, et millegi muutus. Kui aga ainult appideenuse on lubatud annebaasist kirutuda ja lugeda, et küski teine, kui pealneete see kontroler ei tohi otsa annebaasit asi oda, kui lugeda. Kontroler Manager on üldine rakennus, mille sees on mitte muodulid. See on üks pinnaarne projekt, mis saab jooksutada niimoodi, et iga alamoot üleks nuuaks uus protses, aga ta on üks konteiner, mille sees on võimalik aktiveerides erinevad alamootulis, näiteks node controller, mis oluks, et kui pealneete see serveri jälgib, et kas kui pealneete see server on elus või mitte, kas ta on kasutata või mitte, kas tema saab ühendasti mitte ja kas uus server on isatud või teematatud, et ta jälgib kui pealneete see serveri. Job controller siis jälgib töid, et kron-tüüpi töid või praegu peab see jääb jooksma või siis töö opekte, et näiteks loob jobs, et jobsid looban näist poode ja siis jobsid kontrollivad. Põhjuskontrollikas tööd, mida on tehtud kuberneetise jobid, on korrekset välju noot või mitte või siis kordab need. Aga näiteks service accountid hookemkontrollerid saab kuberneetise kontrole loomiseks ka hookema. Ja neid on rohkem, aga idee on, et nad jooksevad ka konteinerit ena ja siis me saame luvu uue kontrollerid mängasid konteineri äld, et ta jookseb node controllerina ja siis temast teha uusi replikad ja aktiveerida neid teised modulid selle tööd. Ja sellist alarmcontrollerid siis tegelevad ningite tegevusta automatiseerimisega klastris. Näite skeduleid on eraldi kontrollitasame teenus, mis siis vaatab, et kas kasutaja on defineerinud, appi kaude defineerinud uue, et mingisugus deploymentid või cron jobid või mingisugused ütleme need statefulsettid või demo-settid ja tema otsustad, mis nootidid peal näid jooksustada, kui on deployment ja deploymentis on kolm replikaseltimidist poodist, millise kui peal näid selle serveride peal kolm poodid ei panna, et tema põhjuselt loeva anno basist, et mis on state ja siis vaatab, et kui uus replik peal looma, siis tema põhjuselt kiinutab anno basid, mis nootid peal see replik peal jooksuma ja tõudab arvase siis ja neide poodid pereid surssida vajadusi, kõidis kui palju melu või see põhjuselt annenpahas anno basi pood soovib, mis on nende hetkel serverides olav liistarva, mis on politika piirangud, mis on tarkkuna piirangud, näiteks, kas poodil on vajad tepud, et siis ka otsustab, et panna selle poodi peal, mille on TPU vannud. Ka näiteks hetkel olevad resursse, kas poodi peal on või kas nootid peal on piisavalt melu või TPU-t siin uut poodi panna. Näiteks, et võib olla mingisugune disk-type võrdud SST vajadus, et anno bas tahab kasutada noode, millel on SST kasutada, et ei oleks nagu aeglane ketas näiteks. Ja saab arvaslata annete lokaalsust, kui mingisugune pood tahab kasutada voljumi, mis siis teerib kaustana teatud serveri peal, siis see pood panas selle serveri peal tööle muidulteta jooksutada tekinti saakist. Ja on ka igas tähpäeva, et kas saab tähpääge et arvaslata. Iga sõlma peal jookseb QBlock ja QProxy. Ja QBlock on siis peamine sõlma akent, mis räägib appiga ja võtab kuulub, et mis selle noodipeal peab tegema, et kas on uus pood, mida peab käebitama selle noodipealt. Ja tema tõmme pallaneid poodspekki või kiretused, mis konteiner peab siis jooksutama appigaudu ja lihtsalt käibitab neid kasutades lokaalsed selle konteinerahaldusrakendus kas tokerit või konteineradeed. Ja ta pidevad jäägib, kas poodid jooksevad. Kui poodid pärslivad, siis taastab neid poodid, et ta vaatab sellist sovidad olukordad, mis peab selle noodipeal jooksma ja siis kontrollib, kas on korreks, et jooksevad selle noodipeal nii mitte. Et põhjumselt, et jooksevad kontrolliba appist, mis peab olema ja vaatab, et mis on reaalne olukord ja liigutab reaalselt olukorda sovidad olukora poole. QProxy siis tegeled võrgu liiklusega, et kui me jooksevad sellel masina peal kolm poodi ja tuleb sellesse noodi mingi porti peale liiklus, siis millises poodise liiklus ümber suunatab. Ta võib seda teha aktiivsalt, et ta iseguulab seda porti või ta võib siis näiteks ümber konfigureerida Linuxi, iPad, Apple, et panam saad reegi paik ka. Kui selle portigale tuleb seda tüütbi protopolik liiklus, siis suunase selle liipaadressile ja ta kas konfigureerib nad, siis ümbersoonamse reegled või edastab iseliklust. Et kui ei ole, nagu iPad Apple siit instaleerid, siis ta võib iselisela neid porti puulata ja liiklust otsa ümbersoonata. Aga halda võrgureegi mõnesmaks mängib tuleb juuri ka, et mis liiklus tüütse seda, siis suunata ja suunab ka konteeerid tunnud liiklusse teiste noodid reale. Ja konteeerdahaldust, targ korra on see näiteks Containerd, või siis Docker Engine, tavalselt Docker Engine ise kasutab Containerd, nii et väga ei ole mõnud, et Docker kasutada kuberneete, sest iga on konfigureerid otsa Containerd, aga kui tead, põhjusel on soov, et ikkagi Dockeri targ korra kasutada näiteks kuberneete, sest välisehalduse jaaks, siis mõni kord on mugavam võib-olla arendatel, kes Containerd ei ole kunaid kasutamud. Otsa Dockerid kasutada, aga ültiselt on soovituslik Containerd teed kasutada. Aga lupatud ka kasutada teisi konteeerdahaldust, targ korra on mis toetada seda open container initiative standardis konteeerd, mis seda CRI standardid siis liiglast implementeerilad. Vanasti oli niimoodi, et kuberneetes ise pidi oma kit-havu projektiis tegema adapterid, et kuidas kuberneetes kasutab Docker, kuidas kuberneetes kasutab teisi container-haldust targ korra, aga nüüd on see ära standardiseeritud ja nüüd kuberneetes nõua, et selleks, et meie saaksime teie Docker-haldust targ korra kasutada, peab see Docker-haldust targ korra, et see on tolki, et container-haldust targ korra siis implementeerimisele OCI liidese, et mis on need käsut, mida poimselt kogu ma näedas välja kutsab ja siis targ korra isepäeba implementeerimine kesut. Aga teatud platformi teaks see näites eradi liidese naa containeri haldust targ korra peal. Näiteb üks ammirantis container ranta, mis on pohimõttel Docker Enterprise, tead on tasuline Docker poimselt. Aga üks kõik, mis see on RII liidese implementeerid kuberneetes kasutatud, nii et kui tegitul on välja uue containeriseerimise haldust targ korra, aga siis nemad, kui ne ema implementeerid OCI liidese konteerineete halduseks, kuberneetes saaks ta kasutada. Ja kõik materjselt konteerid haldust targ korra on kasutatud kuberneetes poolt. Ja vaatates uues seda arkitektuurid, siis meil on mingisugused kontroller, teenuset, mis jooksad containerit enam, mis siis haldaud kogu kuberneeteest. Meil on pooblet, coolprox ja mingi containeri haldust targ korra, mis jooksab iga nootipealt. Noodide ülessead, mis on sulkses lihtne, lihti vajab selline inna pari käsku. Ja üste käsku selleks, et liituda pean kuberneetes serveriga või serverte komplektiga. On soovituslik, et nendest neid jooksevad eralde serveritees. Ja nendest on vähevad kaks või kolm serverit, mis haldavad kuberneetes kontrollerid. Aga üldiselt, et kui teil on palju rakenduse jooksutat, teil ei ole, aga suurtklast, et siis piisav ka ühes. Soovituslik on mitte neid samam serveri peale kokku panna. Selleks, et need konteneerid, mida jooksutatakse võrkerete peal, siis ei saaks mõjutuda neid konteneerid, mis on paja, et pilsa kuba näist halaka saaks. Kuberneetes on sisse ehitatud teenaste leidme. Kui teil on näiteks Annabasi service, mine nimi on Postgres-Tenus, siis suvalne konteneer saab kutsuda või saada liiklusse sellele hostneemine, et näiteks konteneer Annabasi-Tenusa nimele. Me saame need Tenuse nimesid kasutada kui hostneemidele. See on väga mugav. Sarna asia on ka Toker Compose-puhul või Toker servisiste puhul, aga sinna on sisse ehitatud ka kormusõi jaatus. Kui Tenus leidab, et on kolm poodi näende leipoliitagi, siis automaalsed jääb kormus näedel ära, me saame hästi lihtsalt replikate arvu muuta ja panna näid samad liput. See on piisalt selleks, et saada hajusel viisil liiklus kõik teile replikatele. Sisse ehitatud kas allestustruumi orkestreerime, et me saame defineerida, et loodaks uusi kaustasid ja neid maunditakse konteinerides või poodidesse, aga selleks tavaselt jääb üles saama eraldi tarkvara. Kui te ei see eraldi tarkvara üles soovida anmebas jaoks tekitada kusivad kaustasid, siis te peate seda üldiselt tegem käsitse. Te peate näiteks ise defineerima, et mis kaustavad Linuxi serverites on, siis anmebas jaoks ja te peate, kupe näitse üldse, nad kus nad kaustav asuvad ja peate käsitse, et kaustavad looma ja konfigureerima, aga sellaseks saab üles seda, kupe näiteks longhorn persistent storage tarkvara, mis ise hakkab looma neid kaustasid ja ise automaatselt konfigureerib või ümber konfigureerib need. See siia luomise kaus ei mahtunud, aga saate ise selle kõrte lugega, et longhorn on ka sellene, mis automaatselt repliceerib need voljumid, nii et kui ühens serveris keegi kogamat kaustalakustab või server ära kaub, näiteks ketas korrupeerub, siis longhorni seeest repliceerib kõik armeded alati on 2-3 koped anmedest ja anmed ei lähe kaduma. Uute versionid automatiseeritud väljas, keegi tagasavõtmine on väga muga, et saab lihtsalt versioni annebaasis appi kaude ära muuta ja kogu see ümber konverteerimine on automatiseeritud, et ta ise ei pea käski jõursutama selleks, et tarkvara või konteinerid neid, sest version uuedud, et hea on ikkagi monitorist vaatata, et kas ümber konverteerimine töötab edukalt või mitte, kas hakkab erral tekima, et tagasi võtta või rolvergi teha, aga põhimõtteliselt on automatiseeritud ja ei pea nii palju ise käski jõursutama. Kui praneetas oskap isega, resursside kasutust atlimeerid, paned poodid seal tööle, mis on rohkem resursse vaba, aga sisse ei ole te foodina ehidatud seda, et ta ümber balanceerib asju. Kui te olete ise juoksma pannud palju resursse, niimoodi, et kõik nõudid on kasutatud, maksimumi, näiteks, mäluvõid CPUs osas, ja siis ta ei hiljane emaldata, näiteks, kõik asjad, mis juoksad ühe nõudid, siis ei toimu asjada ümber balanceerimist. Võib juhtud, et üks nõud jääb natuke ülejakormatuks, kui te ise asju ümber niiguta, aga siis, kui täitsa vaja on käsits asjad ja siis viisab selleks, et oda asja näiteks ära kustutada, ja siis kubernetis panadad tööle teise nõudid peal, aga selleks, et se balanceerima automaalsed tööteks, peab lisatarkkvara kubernetis ees tööle panena. Ja kubernetis ees on hästi palju lisatarkkvara, mida saab üles heada, nii et tegelikult on võimalik hästi palju, nagu ise tead. Siin on sisse ehidatud automaatne skaleeritavus. Kubernetisest saab defineerita sellise asja, nagu horizontal board autoskeeler, mis kubernetisest sisse ehidatud monitorimist jälgib ja oskab näiteks CPU või mälu kasutusse põhjal otsustada, kui palju replikat juurda panemeemata. Et selleks ei pealiselt lisatarkkvara vaja olla. Kui tead soov midagi muud kasutada peal, et CPU või mälu, siis saada ise näiteks promiituse monitorimis üles heada ja custom meetik, et kasutada selleks, et ümber konfigureerida, kunaa replikete arvuma muudetakse. Aga põhimiselt võredust stalkeriga kubernetisest sisse ehidatud sisse automaatne skaleeritavus. See on ka ise tärnelemine system, et näiteks, kui tead mingil hetkel kasutavalt tarkvara liiga palju resursse, siis võib juhtuda, et väheprioriteetsed asjad panakse seisma või kustataks ära. Aga kui see suure resursse kasutus näiteks öösele madalamast, panaks see poodid tagasi. Kui kubernetisest mingagi väheprioriteetsel mõteevaldab, seda iljem paned tagasi, kui sa olukord paraned. Näiteks, kui te ise panate uue serveri juurde ja mingid asjade ei jookstud, selle tõttub vähe resursse oli, siis kubernetisest paljame uuesti töö. Kui resursse tuleb juurde või resursside kasutus vähenad, siis isetervenne ja system balanceerib ennast uuesti. See kasialu agas ei mahtada, aga kuidas te defineerite prioriteete? Kui te on ammebas, te saaltu üelda, et ammebas vajab minimaalselt 1 GB mõlu, aga te ole lupatud kasutada kuni 3 GB mõlu. See on tegelikult halv. Kui teid pane paik, et ammebasil on rangel 3 GB minimum ja maksimum, siis need asjad, mille on lupatuda, kui olla pörstub olehk rohkem mõlu kasutada, kui on limit, on väheprioriteetsed kui need, mille olev väga rangelimik. Kui peadneedse on parem üelda, et rangel paikapane ammebasil on 3 GB minimum ja maksimum 3 GB, et seda tüüpi teenu seda kõrgema prioriteetiga. Ja need, mitse on näiteks väikse limiitikajaga lupatud, näid 3-4 kod rohkem mõlu või 3 GB mõud kasutada, neid on väikseha prioriteetiga ja need, kui peadneedse eemadab enna. See on kuvitava asja, mis on sisse ehitadud ja peidatud. Mõnesmõttes on hea ammebasile lubata rohkem mõlu kasutada, aga tegelikult mõnesmõttes on see halb, kuna kui peadneedses eemadab neid enne kui teistneed, alati on hea pana fikseeritud mõlu kasutus. Ja teine asjumise halb panan mitte üldse ei pana paikamälu kasutus või sietukasutus konteenerile, kui peadneedse peaks alati panema limidid. Need üleleil on üldse midagi määrahtud, on ka madalama prioriteetiga. Ja on hea ka, et sisse eetadud saladust ja konfiguratsiooni haldus, et saab, nagu ku pereadse see eskirut, näites poskressi paroolid ja hiljem itselt mingis poskressi manifestis linkida nendel, et võtta siit saadusad. Ja need saadust on ka namespeiside speciifiliselt, et me saame anne paasi namespeisiluua saladuse ja nüüd see on poskress saladus. Ja siis kui selles namespeis, siis paneme üles poskressi poodi, siis ta ja Anna ütleb poskress poodi manifestiselt, et võtta saladus sealt. Siis on nagu lihtne hiljem, kas saladust näiks muuta ja poodile kustutada ja poodi uuestil uue selleks, et ta saladus uuesti saab vajaks. Et saab saladusi ka muuta hästi ja need halata. Kes kellevil on küsimusi? Posk või see, kubernets on selline tenoloogia, mida me selles ainees praaktikult läbi ei tee. On pilvetehnoloogias ja sisteemihaalduses kubernets kasutuses, on ka DevOps ainees kubernets kasutuses ja siis sugi seal on eraldi kubernets aine ja see on peanud, et ma pistuid pealitele. Ja väike uuesti kokku võtta selle kubernets ja dockeri võrdlus, et klastritehalduse ja repütsieeristud klastvi üleselt teenusid on kubernetses tokerusmormis suhtselt mõlemast toetatud. Teenust on alka leidmine näitele teenuse lineärgi, et ei pea iga-aalduse kasutada, mene on ka mõlemast toetatud. Tokeris on ülliselt puudlus, valvandus, puumiorgeteerimere, et peate käsid kaustasid looma või neid voliumid looma ja konfigureerime annepaasetele. Ja siis hoolitsema sellest, et neid voliumid ei lähe katuma, neid on repütsieeritud ja seda repütsieeritud voliumitehaaldus ka tokeris on pari raskema. Kubernetses on see osaliselt sisse eitatud ja sa hakkakubernetses üleselta neid plonghoode. Versioonid automaatu uendus ja versioonid tagasi tööramine ka tokeris ole hästi toetatud. Selline enese parandamine kubernetses tokesolmise on mõlemast mõlemast mõttes olemas, kubernetses on ka prioriteetid sees, et saate määrata näiteks, et kui resurs on liiga vähe, siis jättada üles alles need resursid, mille on ranga limidid paigas, et näiteks annepaasi teenusele läheks viimasena kinni ja enne emadates niimoodi appi replikete. Saadust ja konfiguratsioonihaldus on ka kubernetses parem ja automaat eskaleerimist tokers ei ole, et ka peab eraldi otsima, et mis tarkvara peab kasutuma selleks, et muuta replikete arvutoker teenuste stokkers formus. Ja väike kokkuvete ka, et mikro teenused võivad lihtsustada halusüsteemide loomist, kuna me saame intimitavalsed komponentte hallata, areldada, uuendada, skaleerida eraldatud ülejansüsteemist. Me ei pea siis nii palju arvesse võtma ülejans mõnolitse süsteemii sisu, et me saame skaleerida näiteks üksiköid mikro teenust vastavalt sellele, ku palju resursse tema vajab ja me ei pea väga arvesse võtma, et mis on teiste modulite resursivajadusel või hetke jõudusevajadus mõnolitses rakendis. Konteinerid on olnud üle pealnused mikro teenuste edukese võimaldevad, sest lihtsustavad rakendiste ja eraldatud keskondadel oomist ja lihtsalt ülesseadmist, et väga lihtne on neiteks demoksis või CSCDs automatiseerida konteineri pildivist ja konteineri ülesseadmist, siis kui näiteks me muudame koodi kitis ja panaks automaatsal tööle mingi kit pipelineis automaatsal pildivis, olev konteineri uuesti ja laeb üles rekistrisse ja näiteks muuda pärasele versiooni kuberneete seha apikaudu, et mis on selle imituiolisioonistne peoksma. Üks ühe asja, nagu ma oleks ettevaadlik, et kui te olete konteineri tästid palju kasutada, siis tihtite ja tõmbate rekistrist alla poskes kool on leitest. See on tegelikult natuke nõhtlik. Leitest on midagi sellist, mis on selles mõttes, et tõmmartakse kõige viimane versioon praegus ja näitele alla. Aga kui tegib näiteks Toker Hubi uus versioon, siis Toker ja kuberneete ei tõmba uuesti uut versiooni alla. Et jääb ikka sellise, mis oli viimati, kui ta esimest korda jooksutat, et tõmad pange üles, neiks poskras leitest, siis ikkagi annem või sinna kuberneete, siis jääb ikkagi selle version, mis sellel hetkel oli ja ei ole automatiseeritud, kui leitest muutub siis tõmba uuesti alla. Parem on juhtida täpsemalt, mis on see versioon, mida te üles jate, et jõudame poskras 3.7 ja siis järgmine nädal vaatate, kas on uus versiooni, siis muudat äraad nüüd peks on 3.8. Mis juhtub see, et kui te leitest kasutate, siis näiteks production keskkonnast kasutad leitest, testkonnast kasutad leitest ja kuna sellel ühen ädal on vahe, siis mõlemus on erina versioon. Ja teine asja on see, et te unustat äraad, ma panin leitest versiooni üles, et unustat äraad, et tegelikult ei ole leitest, nad ka sajapärast, et siis ta on mingi vanaversioon, aga siis ka ei teada, mis versioon ta oli, sest teempleidis on kirjas leitest. Et te isegi ei saa luge, et misse versioon seal oli, kuna seal on kirjas leitest, et te tead isegi poodi sisse vaatama, et mista oli. Ja mõnikord te isegi poodi sisse vaatatast tead, et te peate vaatama, et mis reaaliselt on see inidž, mis on alla tõmatud sellesse nõudi ja võib teadud olukord, et te tegid isegi see olukord, et te paete leitest poskress üles selle nõudi peal ja siis lisate järgmine uue nõudi kuba neetesesse ja tema tõmab ka poskress leitest jalla, ja see on versioonid eri nõud. Sest üks kuba neetesese nõud pandi üle sellene nädal, üks kuba neetesese nõud pandi see nädal, ja mõle nad tõmbad leitest jalla, aga nad tõmbasid eri nõud lojal leitest jalla. Togger Hubi sisse eidatud see leitestjääg on natuke nõud ohvit teadud olukordades. Ja on tihti tegind olukord, et te ei arvat, et praktikumis näiteks te uuendasid Togger Hubis oma versiooni ja te teete konteineris Togger Service näiteks teete restarti oma konteinelele ja vaatad, et miks tarkvarajal uuen. Aga te reaalselt ei ole Togger Hubist uuesti leitestjit allatõmanud. Leitest versioone allatõmanud. Ja seda Togger Pull poskress leitest käsku seda ei kehta automaassalt uuesti. Ja et tegib sellise synchroniseerimesi probleemid, et alati olge natuke ettevaatlik nende konteineri versioonidega rekistitest. Eest parem on ikkagi kasutada konkreessid versioone. Iga uuendas ajal suurendada versiooni numbrid, et siis tead, mis versioonid te reaalselt kasutate. Ja te saada karanteerid, et mitte erinele serverist tõmati sama versioon alla ja ei tegi seda võimalik olukord, et te tegab erinele versioone. Et mul endel ka vihtun selle ka probleemitekki, et ei tule pähese, et te leitest tagavalt peidus alla täiesti erinele versioon. Ja ma olen näinud, et tudenitelt isti on mõningord väga raskate teete akitelat olukord, kuna ei saa aru, et tegelikult ma kõik luuendasin image-it. Aga mul see image on juba allatõmatud, aga reaalselt see image versioon on erinev, kuna taustas küll failid muutust, aga mõlemad on sama versioon, miin imin leidest. Ja kui docker-sformid võimalda lua suurame docker-klastreid, aga see on raske teatada asja automatiseerida, et kui perneetes lihtsustab konteened ja mikul pärast haldust, aga kõige rohkem peab automatiseerist. Kui teist, docker-s me saame üles näiteks seada Postgres nodeid ja saame docker-sformid üles seada Postgres klastri, kui perneetes me saame saada üles Postgres body, Postgres stateful setid, aga meil on tegelikult võimalik, kas sellase meil näiteks allatõmata kui perneetesest packaged Postgresi Helm template, kus defineeriteks ära, kudas üles seada Postgresi 7-nodilis klastrit või x-nodilis klastrit. Ja siis kõik konfigureeriteks see meile automaatsed, et me isegi peab template isemuutma, et lihtsam on näiteks tõmmat alla Helm template, mis defineerib ära, kudas üles seada Postgres klastrit, mille on 3x nodeid ja teise ei peab selle peale väga palju nõtlema. Aga kui perneetes on veel üks automatiseeris, mis saame kõrgemale, et on sellist asja nagu operaatoreid. Kes on kuulud kui perneetes operaatoreid osta? Mis nad on? Ja millest on see, et see saab selle kui perneetes üles? Võimsalt tiisab kui perneetes üles saab liisest Helmist, et Helm template on tiisab selleks, et üles seada. Et operaatoreid tegemikrataaks on kalli põrgema. Ütlem, et meil on näiteks töötaja, kes on Postgresi speciaalist ja teab, kudas hästi Postgres klastrit halata, kudas konfigureerita automaatses ka leeris või back-up ja midagi. Operaatore mõtta automatiseerida tegevuse, mis toimub väravst ülesseada. Mitte ainult, kuidas ülesseada. Üldjuul piisab Helm template selleks, et asi ülesseada. Aga kui sa tahad automatiseerida hilisemad asjut, siis operaatoreid on starkvara, mis jätkab jooksnud, kui perneetes sees, et Postgres operaatore ei ole Postgres. Postgres operaatore on näiteks koos kirjutatud või pyütonis kirjutatud, mitte ülesseada ja jäibida selle laadud jooksad. Ja näiteks jäibida monitorimist ja otsustada, kas midagi ümber konfigureerida. Operaatoreid on lihtsalt sisseleibadud, mist toimub väravst seda, kui asja on üleval. Ja sinna on palju rohkem asja automatiseerida. Kas või see, kuna poode juurde panna, kuna poode eemal, kuna back upi teha, mida teha enne back upi, mida teha pärast back upi. Ja kõik sellist asjad saab sisse automatiseerida. Võimest, automatiseerimise program, mis jätkab jooks vist kui perneetes sees. Aga tõesti sellel juba oleme üleaja, nii et see läks väga aega jõule. Aga soovitavalt juul to lugega operaatoreid kohtal päris huvitavad, mis kõik seal võimalik on. Aga tõesti operaatoreid on väga mugav viiskuda seda, et see poskust ülesjääda. Mikrotelusest samas võivad teha systemi palju keerukamaks ja ka kallimaks. Kogu systemi võib olla keeruisen, nagu hõlmata ja hallata kui lihtsamad monolitsed systemi. Aga ta võimaladab just rohkem lihtsamine arendada ja skaleerida just teene. Mikrotelusest võivad ka olla ka ega tegi kisemat, sest komponentide vahel nüüd aga tõesti kasutama lokaalsavõrku. Olmuse siis kas GRTC või HATTP protokoll või mingisamiselt töödejärjekorrad. Kõik töötavad üle lokaalsavõrku. Kui sõrvumite saadmine üksteisega suhtlus, võib toimuda üle lokaalsavõrku. See on tähtis, kui mikroteluste komponentid oma vahel suhtlevad. Kui me lihtsalt oma rahmatate halvsa api ja kakama kaheks konteineriteks liiklus tuleb ainult kannutatelt, siis väga seda pahet ei ole. Kui meie apid omavale ei räägi, siis võib olla teda efektiivsest ei kaota külse, sest liiklus ka klientid ja meie systemi vahel nii ülemõrku toimuma. Aga kui me jautavad mingiselt komponentid erineeteks mikrotelusteks, mis oma vahel suhtlevad, siis suhtlus toitab näluasemel näiteks. Sela asem, et on üks suhtlusi funktioni väljakutsuvad, nüüd pead saadma sõrvumite üle võrguni, et kindlasti ta hakkab olema natka aegada. Nii juhetki juba endusti, et kusitame seda asude proov storet siin. Jah, et see on mõneseltes seljetud, et me soovime küsivalt ja mahukalt skaleeritavalt jäägad salastavammed. Olukörras, kus ta sinu arvutis kannutav asuret, on ta kindlasti tohutu taeglase, aga olukörras, kus me paneme ta nähe asuras üles hiljem, siis väga seda pahet võib olla. Ja tõesti lokaalne ja võrgufailisisteem on erinev. Nii asuvad kusagi mujal, et asuvad kuskil lokaalsas võrgus, võib olla kõrval serveredas või teises väkis. Et on tõesti aeglase, kui ta oleks täpselt samas ketas, kui see virtuaalmasi nõuks. Aga praegu ei ole veel pilve üles kannud, nii et praegu on tõesti päris aeglan olla, kui ta sinu arvutis jooksad ja asuras saad juba süsvelast. No pilvas ka ei puhub, siis olla samas, see on kui olub kui piirub. Üldiselt ikkagi huoliselteks see sellest, et oleks effektiivsem, et ei oleks väga aeglen nähtud. Ma ise ei ole uurinud väga benchmark, mis ta mõõdavad, et kui aeglase näid võrgu. Näiteks ma panen liemi üles ja kasutan lokaalselt file versus selleket, et on palem peab asura vahe storias, võib olla leijad mõna avtikli, mis seda näiteks võrdleb. Ma ise ei ole nii, et ise uurinud. Aga kindlasti võrg on aeglase võib lokaalne failisisteeme meel. Eriti Linux tilti hoiab faili nagu mäljusse mäpitult, kui neid loetakse ja kasutab mäljul käsina, et vältida otsaja failisisteemist tugemus. Nii et lokaalselt faili on lihtne mäljumatide ja peab olev piirem nii tugega. Aga see ongi tänaseks siis kõik. Praaktikumis siis see nädal jätkame sellel kahe mikrotenus arendus, aga teen meile natuke ümber ja panen meie fonttelid ette. Ja teen ühe appi metodi katsud suurda. Nägemist!